[
  {
    "context_id": "recursion_1",
    "paragraph": "Any function which calls itself is called recursive. A recursive method solves a problem by calling a copy of itself to work on a smaller problem. This is called the recursion step. The recursion step can result in many more such recursive calls.\n\nIt is important to ensure that the recursion terminates. Each time the function calls itself with a slightly simpler version of the original problem. The sequence of smaller problems must eventually converge on the base case.\n\nRecursion is a useful technique borrowed from mathematics. Recursive code is generally shorter and easier to write than iterative code. Generally, loops are turned into recursive functions when they are compiled or interpreted.\n\nRecursion is most useful for tasks that can be defined in terms of similar subtasks. For example, sort, search, and traversal problems often have simple recursive solutions.\n\nA recursive function performs a task in part by calling itself to perform the subtasks. At some point, the function encounters a subtask that it can perform without calling itself. This case, where the function does not recur, is called the base case. The former, where the function calls itself to perform a subtask, is referred to as the recursive case.\n\nWe can write all recursive functions using the following format:\n\nreturn_type function(parameters) {\n    if (base condition) {\n        return base value;\n    }\n    return recursive expression involving function(modified parameters);\n}\n\nEach recursive call makes a new copy of that method (actually only the variables) in memory. Once a method ends (that is, returns some data), the copy of that returning method is removed from memory. Recursive solutions look simple but visualization and tracing take time.\n\nFor better understanding, consider the following example:\n\nint Print(int n) {\n    if (n == 0)\n        return 0;\n    else {\n        printf(\"%d\", n);\n        return Print(n - 1);\n    }\n}\n\nWhile discussing recursion, the basic question that comes to mind is: which way is better—iteration or recursion? The answer depends on what we are trying to do. A recursive approach mirrors the structure of the problem being solved and can make complex problems easier to formulate. However, recursion adds overhead for each recursive call due to stack frame usage.\n\nRecursion:\n• Terminates when a base case is reached.\n• Each recursive call requires extra space on the stack frame (memory).\n• Infinite recursion may result in stack overflow.\n• Some problems are easier to solve recursively.\n\nIteration:\n• Terminates when a loop condition becomes false.\n• Does not require extra memory per iteration.\n• Infinite loops may run forever without memory overflow.\n• Iterative solutions may not always be as intuitive as recursive ones.\n\nNotes on Recursion:\n• Recursive algorithms have two types of cases: recursive cases and base cases.\n• Every recursive function must terminate at a base case.\n• Iterative solutions are generally more efficient due to lower overhead.\n• Any recursive algorithm can be implemented iteratively using a stack, but it is often not worth the effort.\n• Some problems have no obvious iterative solution.\n• Some problems are better suited to recursion than iteration.\n\nExample Algorithms Using Recursion:\n• Fibonacci series, factorial computation\n• Merge sort, quick sort\n• Binary search\n• Tree traversals (Inorder, Preorder, Postorder)\n• Graph traversals (DFS and BFS)\n• Dynamic programming problems\n• Divide and conquer algorithms\n• Towers of Hanoi\n• Backtracking algorithms"
  }
,
  {
    "context_id": "sorting_1",
    "paragraph": "10.1 What is Sorting?\nSorting is an algorithm that arranges the elements of a list in a certain order (either ascending or descending). The output is a permutation or reordering of the input.\n\n10.2 Why is Sorting Necessary?\nSorting is one of the important categories of algorithms in computer science and a lot of research has gone into this category. Sorting can significantly reduce the complexity of a problem and is often used for database algorithms and searching.\n\n10.3 Classification of Sorting Algorithms\nSorting algorithms are generally categorized based on the following parameters:\n\nBy Number of Comparisons:\nIn this method, sorting algorithms are classified based on the number of comparisons. For comparison-based sorting algorithms, best-case behavior is O(n log n) and worst-case behavior is O(n²). These algorithms evaluate elements using key comparison operations and require at least O(n log n) comparisons for most inputs.\n\nLater in this chapter, non-comparison (linear) sorting algorithms such as Counting Sort, Bucket Sort, and Radix Sort are discussed. Linear sorting algorithms impose restrictions on the input to improve complexity.\n\nBy Number of Swaps:\nSorting algorithms are categorized based on the number of swaps (also called inversions).\n\nBy Memory Usage:\nSome sorting algorithms are in-place and require O(1) or O(log n) auxiliary memory for temporary storage during sorting.\n\nBy Recursion:\nSorting algorithms can be recursive (Quick Sort) or non-recursive (Selection Sort, Insertion Sort). Some algorithms use both approaches, such as Merge Sort.\n\nBy Stability:\nA sorting algorithm is stable if, for all indices i and j such that key A[i] equals key A[j], and record R[i] precedes R[j] in the original list, then R[i] also precedes R[j] in the sorted list. Stable algorithms maintain the relative order of equal elements.\n\nBy Adaptability:\nIn adaptive sorting algorithms, the running time depends on the pre-sortedness of the input. Algorithms like Quick Sort can take advantage of partially sorted data.\n\n10.4 Other Classifications\nAnother method of classifying sorting algorithms is:\n• Internal Sort\n• External Sort\n\nInternal Sort:\nSorting algorithms that use main memory exclusively during sorting are called internal sorting algorithms. These algorithms assume fast random access to memory.\n\nExternal Sort:\nSorting algorithms that use external memory, such as disks or tapes, during sorting are called external sorting algorithms.\n\n10.5 Bubble Sort\nBubble Sort is the simplest sorting algorithm. It works by repeatedly traversing the input array, comparing adjacent elements and swapping them if required. The process continues until no more swaps are needed. Smaller elements gradually bubble to the top of the list.\n\nThe major advantage of Bubble Sort is that it can detect whether the input list is already sorted.\n\nImplementation:\n\nvoid bubbleSort(int arr[], int n) {\n    for (int i = 0; i < n - 1; i++) {\n        for (int j = 0; j < n - i - 1; j++) {\n            if (arr[j] > arr[j + 1]) {\n                int temp = arr[j];\n                arr[j] = arr[j + 1];\n                arr[j + 1] = temp;\n            }\n        }\n    }\n}\n\nThe basic Bubble Sort algorithm takes O(n²) time even in the best case. It can be improved using a flag to stop early when no swaps occur. This improves the best-case time complexity to O(n).\n\nPerformance:\nWorst case: O(n²)\nBest case (optimized): O(n)\nAverage case: O(n²)\nSpace complexity: O(1) auxiliary\n\n10.6 Selection Sort\nSelection Sort is an in-place sorting algorithm. It repeatedly selects the minimum element from the unsorted portion and places it at the correct position.\n\nAdvantages:\n• Easy to implement\n• In-place sorting\n\nDisadvantages:\n• Poor scalability: O(n²)\n\nAlgorithm:\n1. Find the minimum element in the list\n2. Swap it with the current position\n3. Repeat until the array is sorted\n\nImplementation:\n\nvoid selectionSort(int arr[], int n) {\n    for (int i = 0; i < n - 1; i++) {\n        int min = i;\n        for (int j = i + 1; j < n; j++) {\n            if (arr[j] < arr[min])\n                min = j;\n        }\n        if (min != i) {\n            int temp = arr[i];\n            arr[i] = arr[min];\n            arr[min] = temp;\n        }\n    }\n}\n\nPerformance:\nWorst case: O(n²)\nBest case: O(n²)\nAverage case: O(n²)\nSpace complexity: O(1) auxiliary\n\n10.7 Insertion Sort\nInsertion Sort is a simple and efficient comparison-based sorting algorithm. In each iteration, an element is removed from the input and inserted into the correct position in the sorted portion of the list.\n\nAdvantages:\n• Simple implementation\n• Efficient for small datasets\n• Adaptive to nearly sorted data\n• Stable\n• In-place\n• Online algorithm\n\nAlgorithm:\nEach iteration removes an element and inserts it into the correct position in the sorted portion. After k iterations, the first k + 1 elements are sorted.\n\nImplementation:\n\nvoid insertionSort(int arr[], int n) {\n    for (int i = 1; i < n; i++) {\n        int key = arr[i];\n        int j = i - 1;\n        while (j >= 0 && arr[j] > key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n"
  }
,
{
  "context_id": "sorting_3",
  "paragraph": "10.8 Analysis of Insertion Sort\nWorst Case Analysis\nWorst case occurs when for every i the inner loop has to move all elements A[1], ..., A[i−1].\nAverage Case Analysis\nOn average, the key is inserted in the middle of the sorted portion.\nPerformance\nBest case: Θ(n) when array is already sorted.\nWorst case: Θ(n²)\nAverage case: Θ(n²)\nSpace complexity: O(1) auxiliary\nImplementation\nvoid insertionSort(int arr[], int n) {\n    for (int i = 1; i < n; i++) {\n        int key = arr[i];\n        int j = i - 1;\n        while (j >= 0 && arr[j] > key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\n10.9 Shell Sort\nShell sort is a generalization of insertion sort using gap sequences.\nImplementation\nvoid shellSort(int arr[], int n) {\n    for (int gap = n / 2; gap > 0; gap /= 2) {\n        for (int i = gap; i < n; i++) {\n            int temp = arr[i];\n            int j;\n            for (j = i; j >= gap && arr[j - gap] > temp; j -= gap)\n                arr[j] = arr[j - gap];\n            arr[j] = temp;\n        }\n    }\n}\nPerformance\nWorst case depends on gap sequence.\nBest case: O(n)\n\n10.10 Merge Sort\nMerge sort follows divide and conquer.\nImplementation\nvoid merge(int arr[], int l, int m, int r) {\n    int n1 = m - l + 1, n2 = r - m;\n    int L[n1], R[n2];\n    for (int i = 0; i < n1; i++) L[i] = arr[l + i];\n    for (int j = 0; j < n2; j++) R[j] = arr[m + 1 + j];\n    int i = 0, j = 0, k = l;\n    while (i < n1 && j < n2)\n        arr[k++] = (L[i] <= R[j]) ? L[i++] : R[j++];\n    while (i < n1) arr[k++] = L[i++];\n    while (j < n2) arr[k++] = R[j++];\n}\n\nvoid mergeSort(int arr[], int l, int r) {\n    if (l < r) {\n        int m = l + (r - l) / 2;\n        mergeSort(arr, l, m);\n        mergeSort(arr, m + 1, r);\n        merge(arr, l, m, r);\n    }\n}\nPerformance\nWorst/Average/Best case: Θ(n log n)\nSpace: Θ(n)\n\n10.11 Heap Sort\nHeap sort is an in-place comparison-based sorting algorithm.\nImplementation\nvoid heapify(int arr[], int n, int i) {\n    int largest = i;\n    int l = 2 * i + 1;\n    int r = 2 * i + 2;\n    if (l < n && arr[l] > arr[largest]) largest = l;\n    if (r < n && arr[r] > arr[largest]) largest = r;\n    if (largest != i) {\n        int temp = arr[i];\n        arr[i] = arr[largest];\n        arr[largest] = temp;\n        heapify(arr, n, largest);\n    }\n}\n\nvoid heapSort(int arr[], int n) {\n    for (int i = n / 2 - 1; i >= 0; i--)\n        heapify(arr, n, i);\n    for (int i = n - 1; i > 0; i--) {\n        int temp = arr[0];\n        arr[0] = arr[i];\n        arr[i] = temp;\n        heapify(arr, i, 0);\n    }\n}\n\n10.12 Quick Sort\nQuick sort partitions the array around a pivot.\nImplementation\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low - 1;\n    for (int j = low; j < high; j++) {\n        if (arr[j] < pivot) {\n            i++;\n            int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;\n        }\n    }\n    int temp = arr[i + 1]; arr[i + 1] = arr[high]; arr[high] = temp;\n    return i + 1;\n}\n\nvoid quickSort(int arr[], int low, int high) {\n    if (low < high) {\n        int pi = partition(arr, low, high);\n        quickSort(arr, low, pi - 1);\n        quickSort(arr, pi + 1, high);\n    }\n}\nPerformance\nBest/Average: O(n log n)\nWorst: O(n²)\n\n10.13 Counting Sort\nCounting sort is a non-comparison sort.\nImplementation\nvoid countingSort(int arr[], int n, int k) {\n    int count[k + 1];\n    int output[n];\n    for (int i = 0; i <= k; i++) count[i] = 0;\n    for (int i = 0; i < n; i++) count[arr[i]]++;\n    for (int i = 1; i <= k; i++) count[i] += count[i - 1];\n    for (int i = n - 1; i >= 0; i--)\n        output[--count[arr[i]]] = arr[i];\n    for (int i = 0; i < n; i++) arr[i] = output[i];\n}\n\n10.14 Radix Sort\nRadix sort processes digits using counting sort.\nPerformance\nTime complexity: O(nd)\n\n10.15 External Sorting\nExternal sorting is used when data does not fit into main memory. External merge sort is commonly used."
}]

